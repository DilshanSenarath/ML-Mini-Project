{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":4702.204495,"end_time":"2022-05-30T19:08:51.396448","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-05-30T17:50:29.191953","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nimport cupy, cudf # GPU LIBRARIES\nimport numpy as np, pandas as pd # CPU LIBRARIES\nimport matplotlib.pyplot as plt, gc\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Limit GPU Memory\nLIMIT = 8\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n  except RuntimeError as e:\n    print(e)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":8.151909,"end_time":"2022-05-30T17:50:45.831906","exception":false,"start_time":"2022-05-30T17:50:37.679997","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial Data","metadata":{}},{"cell_type":"code","source":"cat_columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reduce the size of training data and feature engineering (I am not using all the feature engineering techniques which have been used in KNN, SVM, and CatBoost in this model)","metadata":{"papermill":{"duration":0.005339,"end_time":"2022-05-30T17:50:45.842915","exception":false,"start_time":"2022-05-30T17:50:45.837576","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train labels\ntargets = cudf.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv')\ntargets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n\n# All column names\ntrain = cudf.read_csv('/kaggle/input/amex-default-prediction/train_data.csv', nrows=1)\ntot_columns  = train.columns","metadata":{"_kg_hide-input":true,"papermill":{"duration":7.818119,"end_time":"2022-05-30T17:50:53.684989","exception":false,"start_time":"2022-05-30T17:50:45.866870","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_eng(train, targets = None):\n    # Reduce non feature columns\n    train['customer_ID'] = train['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n    train.S_2 = cudf.to_datetime(train.S_2)\n    train['year'] = (train.S_2.dt.year-2000).astype('int8')\n    train['month'] = (train.S_2.dt.month).astype('int8')\n    train['day'] = (train.S_2.dt.day).astype('int8')\n    # Delete the raw date time field\n    del train['S_2']\n        \n    # label encoding\n    d_63_catgories = {'CL':2, 'CO':3, 'CR':4, 'XL':5, 'XM':6, 'XZ':7}\n    train['D_63'] = train.D_63.map(d_63_catgories).fillna(1).astype('int8')\n    d_64_catgories = {'-1':2,'O':3, 'R':4, 'U':5}\n    train['D_64'] = train.D_64.map(d_64_catgories).fillna(1).astype('int8')\n    adding_val = [2,1,2,2,3,2,3,2,2]\n    for c,s in zip(list(set(cat_columns)-{'D_63','D_64'}), adding_val):\n        train[c] = train[c] + s\n        train[c] = train[c].fillna(1).astype('int8')\n    \n    # Reduce size of other columns\n    skips = ['customer_ID','year','month','day']\n    for col in train.columns:\n        if col in skips: continue\n        if str(train[col].dtype)=='int64':\n            train[col] = train[col].astype('int32')\n        if str(train[col].dtype)=='float64':\n            train[col] = train[col].astype('float32')\n    \n    # Padding the sequences as 13 samples\n    tmp = train[['customer_ID']].groupby('customer_ID').customer_ID.agg('count')\n    more = cupy.array([],dtype='int64') \n    for j in range(1,13):\n        i = tmp.loc[tmp==j].index.values\n        more = cupy.concatenate([more,cupy.repeat(i,13-j)])\n    df = train.iloc[:len(more)].copy().fillna(0)\n    # Padding numerical columns with -1\n    df = df * 0 - 1\n    # Padding categorical columns with 0\n    df[cat_columns] = (df[cat_columns] * 0).astype('int8')\n    df['customer_ID'] = more\n    train = cudf.concat([train,df],axis=0,ignore_index=True)\n        \n    # Integrate the targets\n    if targets is not None:\n        train = train.merge(targets,on='customer_ID',how='left')\n        train.target = train.target.astype('int8')\n        \n    # Handle missing values\n    for col in train.columns:\n        if col not in skips + cat_columns:\n            train[col] = train[[col]].fillna((train[[col]].median()).astype('float32'))\n    \n    # Sort\n    train = train.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n    \n    # Drop non required columns\n    train = train.drop(['year','month','day'],axis=1)\n    \n    # Rearrange the categorical coluns to the begining of the data frame\n    temp = list(train.columns[1:])\n    temp = ['customer_ID'] + cat_columns + [col for col in temp if col not in cat_columns]\n    train = train[temp]\n    \n    return train","metadata":{"papermill":{"duration":0.026423,"end_time":"2022-05-30T17:50:54.305611","exception":false,"start_time":"2022-05-30T17:50:54.279188","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_customers = pd.read_csv('/kaggle/input/amex-default-prediction/train_data.csv', usecols=['customer_ID'])\ncustomers = all_customers.drop_duplicates().sort_index().values.flatten()\nrows = []\neach_raw_count = len(customers) // 10 \nfor k in range(10):\n    if (k==9):\n        part = customers[k*each_raw_count:]\n    else: \n        part = customers[k*each_raw_count:(k+1)*each_raw_count]\n    count = all_customers.loc[all_customers.customer_ID.isin(part)].shape[0]\n    rows.append(count)\nrows","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = [553403, 552855, 554025, 554330, 552004, 552378, 552822, 553151, 553493, 552990] # Found from above cell code (It is time consuming)\n# Create train files => 10 files     \nfor k in range(10):\n    skip = int(np.sum(rows[:k]) + 1)\n    train = cudf.read_csv('/kaggle/input/amex-default-prediction/train_data.csv', nrows=rows[k], skiprows=skip, header=None, names=tot_columns)\n\n    train = feature_eng(train, targets = targets)\n    print(train)\n    tar = train[['customer_ID','target']].drop_duplicates().sort_index()\n    tar.to_parquet(f'targets_{k+1}.pqt',index=False)\n    data = train.iloc[:,1:-1].values.reshape((-1,13,188))\n    cupy.save(f'data_{k+1}',data.astype('float32'))\n\ndel train, tar, data\ndel targets\ngc.collect()","metadata":{"_kg_hide-input":true,"papermill":{"duration":413.540875,"end_time":"2022-05-30T17:57:47.852255","exception":false,"start_time":"2022-05-30T17:50:54.311380","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.010301,"end_time":"2022-05-30T17:57:47.880213","exception":false,"start_time":"2022-05-30T17:57:47.869912","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# GRU Model\ndef build_model():\n    inp = tf.keras.Input(shape=(13,188))\n    embeddings = []\n    for k in range(11):\n        emb = tf.keras.layers.Embedding(10,4)\n        embeddings.append(emb(inp[:,:,k]))\n    x = tf.keras.layers.Concatenate()([inp[:,:,11:]]+embeddings)\n\n    # RNN\n    x = tf.keras.layers.GRU(units=128, return_sequences=False)(x)\n    x = tf.keras.layers.Dense(64,activation='relu')(x)\n    x = tf.keras.layers.Dense(32,activation='relu')(x)\n\n    # Output Layer\n    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n    \n    # Compile the model\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    loss = tf.keras.losses.BinaryCrossentropy()\n    model.compile(loss=loss, optimizer = opt)\n    \n    return model","metadata":{"papermill":{"duration":0.0383,"end_time":"2022-05-30T17:57:47.925787","exception":false,"start_time":"2022-05-30T17:57:47.887487","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Learning rate scheduler\ndef lr_s (epoch):\n    lr = [1e-3]*5 + [1e-4]*2 + [1e-5]*1\n    return lr[epoch]\nlearning_rate_callback = tf.keras.callbacks.LearningRateScheduler(lr_s, verbose = False)","metadata":{"papermill":{"duration":0.020396,"end_time":"2022-05-30T17:57:47.956835","exception":false,"start_time":"2022-05-30T17:57:47.936439","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Metirc","metadata":{}},{"cell_type":"code","source":"# COMPETITION METRIC FROM Konstantin Yakovlev\n# https://www.kaggle.com/kyakovlev\n# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\ndef amex_metric_mod(y_true, y_pred):\n\n    labels     = np.transpose(np.array([y_true, y_pred]))\n    labels     = labels[labels[:, 1].argsort()[::-1]]\n    weights    = np.where(labels[:,0]==0, 20, 1)\n    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n\n    gini = [0,0]\n    for i in [1,0]:\n        labels         = np.transpose(np.array([y_true, y_pred]))\n        labels         = labels[labels[:, i].argsort()[::-1]]\n        weight         = np.where(labels[:,0]==0, 20, 1)\n        weight_random  = np.cumsum(weight / np.sum(weight))\n        total_pos      = np.sum(labels[:, 0] *  weight)\n        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n        lorentz        = cum_pos_found / total_pos\n        gini[i]        = np.sum((lorentz - weight_random) * weight)\n\n    return 0.5 * (gini[1]/gini[0] + top_four)","metadata":{"papermill":{"duration":0.027183,"end_time":"2022-05-30T17:57:48.016224","exception":false,"start_time":"2022-05-30T17:57:47.989041","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{"papermill":{"duration":0.01094,"end_time":"2022-05-30T17:57:48.037438","exception":false,"start_time":"2022-05-30T17:57:48.026498","status":"completed"},"tags":[]}},{"cell_type":"code","source":"true = np.array([])\noof = np.array([])\nVERBOSE = 1 \n\nfor fold in range(5):\n    # Generate validation and training idx\n    valid_idx = [2 * fold + 1, 2 * fold + 2]\n    train_idx = [x for x in [1,2,3,4,5,6,7,8,9,10] if x not in valid_idx]\n\n    print('='*25)\n    print(f'##### Fold {fold+1} with valid files', valid_idx)\n\n    # Read train data\n    X_train = []; y_train = []\n    for k in train_idx:\n        X_train.append(np.load(f'data_{k}.npy'))\n        y_train.append(pd.read_parquet(f'targets_{k}.pqt') )\n    X_train = np.concatenate(X_train,axis=0)\n    y_train = pd.concat(y_train).target.values\n    print('### Training data shapes', X_train.shape, y_train.shape)\n\n    # Read validation data\n    X_valid = []; y_valid = []\n    for k in valid_idx:\n        X_valid.append( np.load(f'data_{k}.npy'))\n        y_valid.append( pd.read_parquet(f'targets_{k}.pqt') )\n    X_valid = np.concatenate(X_valid,axis=0)\n    y_valid = pd.concat(y_valid).target.values\n    print('### Validation data shapes', X_valid.shape, y_valid.shape)\n    print('='*25)\n\n    # Train model\n    K.clear_session()\n    model = build_model()\n    h = model.fit(X_train,y_train, \n                  validation_data = (X_valid,y_valid),\n                  batch_size=512, epochs=8, verbose=VERBOSE,\n                  callbacks = [learning_rate_callback])\n    model.save_weights(f'gru_fold_{fold+1}.h5')\n\n    # Validate\n    print('Validate...')\n    preds = model.predict(X_valid, batch_size=512, verbose=VERBOSE).flatten()\n\n    print(f'~~~~Fold {fold+1} Result: ', amex_metric_mod(y_valid, preds) )\n    \n    true = np.concatenate([true, y_valid])\n    oof = np.concatenate([oof, preds])\n\n    del model, X_train, y_train, X_valid, y_valid, preds\n    gc.collect()\n\nprint('='*25)\nprint(f'Final Result: ', amex_metric_mod(true, oof))\nprint('='*25)\nK.clear_session()","metadata":{"_kg_hide-input":true,"papermill":{"duration":554.807567,"end_time":"2022-05-30T18:07:02.856272","exception":false,"start_time":"2022-05-30T17:57:48.048705","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{"papermill":{"duration":0.010566,"end_time":"2022-05-30T17:57:47.978369","exception":false,"start_time":"2022-05-30T17:57:47.967803","status":"completed"},"tags":[]}},{"cell_type":"code","source":"all_customers = pd.read_csv('/kaggle/input/amex-default-prediction/test_data.csv', usecols=['customer_ID'])\ncustomers = all_customers.drop_duplicates().sort_index().values.flatten()\nrows = []\neach_raw_count = len(customers) // 20 \nfor k in range(20):\n    if (k==19):\n        part = customers[k*each_raw_count:]\n    else: \n        part = customers[k*each_raw_count:(k+1)*each_raw_count]\n    count = all_customers.loc[all_customers.customer_ID.isin(part)].shape[0]\n    rows.append(count)\nrows","metadata":{"_kg_hide-input":false,"papermill":{"duration":1.394878,"end_time":"2022-05-30T18:07:04.291019","exception":false,"start_time":"2022-05-30T18:07:02.896141","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = [567933,568482,569369,567886,567539,568041,568138,567596,568543,567539,568421,568745,568279,568333,568327,568901,568300,568001,567372,568017] # Get from the above cell code (Time consuming to execute)\ntest_customer_hashes = cupy.array([],dtype='int64')\n\n# Create folds from test data\nfor k in range(20):\n    skip = int(np.sum( rows[:k] ) + 1)\n    test = cudf.read_csv('/kaggle/input/amex-default-prediction/test_data.csv', nrows=rows[k], \n                          skiprows=skip, header=None, names=tot_columns)\n\n    test = feature_eng(test, targets = None)\n\n    customer_idxs = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n    test_customer_hashes = cupy.concatenate([test_customer_hashes,customer_idxs])\n\n    data = test.iloc[:,1:].values.reshape((-1,13,188))\n    cupy.save(f'test_data_{k+1}',data.astype('float32'))\n\ncupy.save(f'test_customer_hashes', test_customer_hashes)\n\ndel test, data\ngc.collect()","metadata":{"_kg_hide-input":true,"papermill":{"duration":3519.263224,"end_time":"2022-05-30T19:05:45.510635","exception":false,"start_time":"2022-05-30T18:07:06.247411","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction Generation\nstart = 0; end = 0\nsub = cudf.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\n\n# Rearrange submission rows\nsub['hash'] = sub['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\ntest_hash_index = cupy.load(f'test_customer_hashes.npy')\nsub = sub.set_index('hash').loc[test_hash_index].reset_index(drop=True)\n\nfor k in range(20):\n    K.clear_session()\n    model = build_model()\n    \n    # Loading test data folds\n    X_test = np.load(f'test_data_{k+1}.npy')\n    end = start + X_test.shape[0]\n\n    # Loading models\n    model.load_weights(f'gru_fold_1.h5')\n    preds = model.predict(X_test, batch_size=512, verbose=0).flatten() \n    for j in range(1,5):\n        model.load_weights(f'gru_fold_{j+1}.h5')\n        preds += model.predict(X_test, batch_size=512, verbose=0).flatten()\n    preds /= 5.0\n\n    sub.loc[start:end-1,'prediction'] = preds\n    start = end\n\n    del model, X_test, preds\n    gc.collect()","metadata":{"_kg_hide-input":true,"papermill":{"duration":181.236439,"end_time":"2022-05-30T19:08:46.790246","exception":false,"start_time":"2022-05-30T19:05:45.553807","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Submission","metadata":{"papermill":{"duration":0.015373,"end_time":"2022-05-30T19:08:46.821435","exception":false,"start_time":"2022-05-30T19:08:46.806062","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sub.to_csv('submission.csv',index=False)\nsub.head()","metadata":{"papermill":{"duration":0.258173,"end_time":"2022-05-30T19:08:47.095245","exception":false,"start_time":"2022-05-30T19:08:46.837072","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}