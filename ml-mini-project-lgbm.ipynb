{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport gc\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom lightgbm import LGBMClassifier\n\n# data viz\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\n\n# styling\nplt.style.use(\"ggplot\")\nrcParams['figure.figsize'] = (12,  6)\n\nRANDOM_SEED = 6    # Set a random seed for reproducibility!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Freed up memory\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial Details","metadata":{}},{"cell_type":"code","source":"cat_columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reduce Dataset Size","metadata":{}},{"cell_type":"code","source":"# Load train labels\ntargets = pd.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv')\ntargets['customer_ID'] = targets['customer_ID'].apply(lambda x: int(x[-16:],16)).astype('int64')\n\nprint(f\"Target Shape: {targets.shape}\")\n\n# Create a parquet file containing train labels\ntargets.to_parquet('/kaggle/working/train_labels.pqt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_size(df, cat_columns):\n    # Reduce date time and customer id column size\n    df['customer_ID'] = df['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n    df.S_2 = pd.to_datetime(df.S_2)\n    df['year'] = (df.S_2.dt.year-2000).astype('int8')\n    df['month'] = (df.S_2.dt.month).astype('int8')\n    df['day'] = (df.S_2.dt.day).astype('int8')\n    del df['S_2']\n    \n    # Reduce categorical column sizes (Apply label encoding)\n    d_63_map = {'CL':2, 'CO':3, 'CR':4, 'XL':5, 'XM':6, 'XZ':7}\n    df['D_63'] = df.D_63.map(d_63_map).fillna(1).astype('int8')\n\n    d_64_map = {'-1':2,'O':3, 'R':4, 'U':5}\n    df['D_64'] = df.D_64.map(d_64_map).fillna(1).astype('int8')\n    \n    cat_int_columns = cat_columns[:7] + cat_columns[-2:]\n    adding_values = [2,1,2,2,3,2,3,2,2]\n    for c,s in zip(cat_int_columns, adding_values):\n        df[c] = df[c] + s\n        df[c] = df[c].fillna(1).astype('int8')\n    \n    # Reduce size of other columns\n    skip_columns = ['customer_ID']\n    for c in df.columns:\n        if c in skip_columns:\n            continue\n        if str(df[c].dtype) == 'int64':\n            df[c] = df[c].astype('int32')\n        if str(df[c].dtype) == 'float64':\n            df[c] = df[c].astype('float32')\n    \n    # Sort dataframe by customer_ID, year, month, day (ascending order)\n    df = df.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load train data reduce size and write to a parquet file\n# chunksize = 500000 rows\npq_writer = None\nfor idx, df in enumerate(pd.read_csv('/kaggle/input/amex-default-prediction/train_data.csv', chunksize=500000)):\n    print(df.shape)\n    df = reduce_size(df, cat_columns)\n    \n    table = pa.Table.from_pandas(df)\n    if (idx == 0):\n        pq_writer = pq.ParquetWriter('/kaggle/working/train_data.pqt', table.schema)\n    pq_writer.write_table(table)\n\nif (pq_writer):\n    pq_writer.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del targets\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load test data reduce size and write to a parquet file\n# chunksize = 3000000 rows\nfor idx, df in enumerate(pd.read_csv('/kaggle/input/amex-default-prediction/test_data.csv', chunksize=3000000)):\n    print(df.shape)\n    df = reduce_size(df, cat_columns)\n    \n    df.to_parquet(f'/kaggle/working/test_data_{idx}.pqt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Prepared Dataset","metadata":{}},{"cell_type":"code","source":"# Load training data\ntrain = pd.read_parquet('/kaggle/working/train_data.pqt')\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load training labels\ntrain_labels = pd.read_parquet('/kaggle/working/train_labels.pqt')\ntrain_labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info(verbose=True, show_counts=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print duplicate rows count\ntrain.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels.target.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels.target.value_counts().plot(kind=\"bar\")\nplt.title(\"Value counts of the target variable\")\nplt.xlabel(\"Default or not\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df = train.merge(train_labels,on='customer_ID',how='left')\ncrr = temp_df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cor_cols = []\ncols = list(crr.columns)\nfor i in range(len(cols)):\n    for j in cols[:i]:\n        if (crr.loc[cols[i]][j] > 0.6):\n            cor_cols.append(j)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cor_cols = set(cor_cols)\nprint(len(cor_cols))\nlist(cor_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nan_percentage = (train.isna().sum()/len(train.index)) * 100\ndrop_columns = []\nfor idx in nan_percentage.index:\n    if (nan_percentage[idx] > 80):\n        drop_columns.append(idx)\ndrop_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"def feature_eng(df, labels, is_train=True):\n    df_local = df.copy()\n    \n    # Drop Columns With High NaN Rates\n    df_local = df_local.drop(columns=['D_42','D_49','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142'])\n    \n    # Remove Highly Correlated Features => threshold = 0.8\n    df_local = df_local.drop(columns=['D_74','B_11','B_18','S_22','D_103','D_104','D_48','D_58','B_1','B_7','S_3','B_12','D_131','D_118','D_141','D_115','B_16','D_139','D_62','B_14','B_2','D_79'])\n    \n    # Handle Missing Values\n    imp_freq = SimpleImputer(strategy='median')\n    skips = cat_columns + ['customer_ID', 'day', 'month', 'year']\n    for col in df_local.columns:\n        if col not in skips:\n            df_local[col] = imp_freq.fit_transform(df_local[[col]])\n            \n    # Scaling\n    scalar = MinMaxScaler()\n    for col in df_local.columns:\n        if col not in skips:\n            df_local[col] = scalar.fit_transform(df_local[[col]])\n            \n    # Convert to int\n    for col in skips:\n        df_local[col] = df_local[col].astype('int8')\n            \n    # Get the average of all ime series data for each customer\n    df_local = df_local.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n    df_local = df_local.groupby('customer_ID').mean()\n    \n    # Remove non-required columns\n    df_local = df_local.drop(columns=['year','month','day'])\n    \n    if is_train:\n        # Join labels and data\n        target_df = labels.sort_values(['customer_ID']).reset_index(drop=True)\n        print(\"Sort Order Correct: \", list(target_df['customer_ID']) == list(df_local.index))\n        df_local[\"target\"] = pd.Series(target_df['target'].to_numpy(), index=df_local.index)\n    \n    return df_local","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = feature_eng(train, train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model implementation","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    train.drop(columns=['target']),\n    train['target'],\n    test_size=0.2,\n    shuffle=True,\n    stratify=train['target'],\n    random_state=RANDOM_SEED\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LGBMClassifier(objective= 'binary')\n# model.fit(train.drop(columns=['target']), train['target'])\nmodel.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# COMPETITION METRIC FROM Konstantin Yakovlev\n# https://www.kaggle.com/kyakovlev\n# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\ndef amex_metric_mod(y_true, y_pred):\n    labels     = np.transpose(np.array([y_true, y_pred]))\n    labels     = labels[labels[:, 1].argsort()[::-1]]\n    weights    = np.where(labels[:,0]==0, 20, 1)\n    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n\n    gini = [0,0]\n    for i in [1,0]:\n        labels         = np.transpose(np.array([y_true, y_pred]))\n        labels         = labels[labels[:, i].argsort()[::-1]]\n        weight         = np.where(labels[:,0]==0, 20, 1)\n        weight_random  = np.cumsum(weight / np.sum(weight))\n        total_pos      = np.sum(labels[:, 0] *  weight)\n        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n        lorentz        = cum_pos_found / total_pos\n        gini[i]        = np.sum((lorentz - weight_random) * weight)\n\n    return 0.5 * (gini[1]/gini[0] + top_four)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict_proba(X_test)\namex_metric_mod(y_test, preds[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate Predictions\npredictions = []\nfor idx in range(4):\n    df = pd.read_parquet(f'/kaggle/working/test_data_{idx}.pqt')\n    test = feature_eng(df, None, is_train=False)\n    preds = model.predict_proba(test)\n    temp_df = pd.DataFrame(np.expand_dims(preds[:,1], axis=1), columns=['preds'])\n    temp_df['mapping'] = list(test.index)\n    temp_df.to_parquet(f'/kaggle/working/pred_{idx}.pqt')\n    predictions.append(temp_df)\n    \n    del df,test,preds,temp_df\n    gc.collect()\n    print(f'Chunk is finished: {idx}')\n\npredictions_df = pd.concat(predictions)\npredictions_df = predictions_df.groupby('mapping', as_index=False).mean()\npredictions_df = predictions_df.sort_values(['mapping']).reset_index(drop=True)\npredictions_df.to_parquet('/kaggle/working/predictions.pqt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df = pd.read_parquet('/kaggle/working/predictions.pqt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\nsub[\"mapping\"] = sub['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\nsub = sub.merge(predictions_df, on='mapping', how='left')\nsub['prediction'] = sub['preds']\nsub = sub.drop(columns=['mapping', 'preds'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model, predictions, predictions_df, sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}